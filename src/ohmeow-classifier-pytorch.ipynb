{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext import vocab, data\n",
    "from torchtext.datasets import language_modeling\n",
    "\n",
    "from fastai.rnn_reg import *\n",
    "from fastai.rnn_train import *\n",
    "from fastai.nlp import *\n",
    "from fastai.lm_rnn import *\n",
    "from fastai.structured import *\n",
    "from fastai import sgdr\n",
    "\n",
    "import spacy\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "import dill as pickle\n",
    "\n",
    "# pandas and plotting config\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'tqdm' has no attribute '_instances'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-fb3a0495b094>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtqdm_cls\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0minst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtqdm_cls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_instances\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'tqdm' has no attribute '_instances'"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm as tqdm_cls\n",
    "inst = tqdm_cls._instances\n",
    "for i in range(len(inst)): inst.pop().close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"C://Users/hafid/Dropbox/3.SelfStudy/kaggle/ai6-kaggle-toxic/data/train/train.csv\")\n",
    "test = pd.read_csv(\"C://Users/hafid/Dropbox/3.SelfStudy/kaggle/ai6-kaggle-toxic/data/test/test.csv\")\n",
    "# subm = pd.read_csv(\"C://Users/hafid\\Dropbox/3.SelfStudy/kaggle/jigsaw-toxic-comment-classification/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='C://Users/hafid/Dropbox/3.SelfStudy/kaggle/ai6-kaggle-toxic/'\n",
    "\n",
    "TRN_PATH = 'data/train/'\n",
    "VAL_PATH = 'data/test/'\n",
    "TRN = f'{PATH}{TRN_PATH}'\n",
    "VAL = f'{PATH}{VAL_PATH}'\n",
    "\n",
    "os.listdir(PATH)\n",
    "\n",
    "train = pd.read_csv(f'{TRN}train.csv')\n",
    "test = pd.read_csv(f'{VAL}test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 159571 | Test size: 153164\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  \\\n",
       "0  0000997932d777bf   \n",
       "1  000103f0d9cfb60f   \n",
       "2  000113f07ec002fd   \n",
       "\n",
       "                                                                                                                                                                                                                                                                comment_text  \\\n",
       "0  Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27   \n",
       "1  D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)                                                                                                                                                            \n",
       "2  Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.                                   \n",
       "\n",
       "   toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0  0      0             0        0       0       0              \n",
       "1  0      0             0        0       0       0              \n",
       "2  0      0             0        0       0       0              "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll ever be whats up with you and hating you sad mofuckas...i should bitch slap ur pethedic white faces and get you to kiss my ass you guys sicken me. Ja rule is about pride in da music man. dont diss that shit on him. and nothin is wrong bein like tupac he was a brother too...fuckin white boys get things right next time.,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is, IMO.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lapland —  /  \"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  \\\n",
       "0  00001cee341fdb12   \n",
       "1  0000247867823ef7   \n",
       "2  00013b17ad220c46   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                      comment_text  \n",
       "0  Yo bitch Ja Rule is more succesful then you'll ever be whats up with you and hating you sad mofuckas...i should bitch slap ur pethedic white faces and get you to kiss my ass you guys sicken me. Ja rule is about pride in da music man. dont diss that shit on him. and nothin is wrong bein like tupac he was a brother too...fuckin white boys get things right next time.,  \n",
       "1  == From RfC == \\n\\n The title is fine as it is, IMO.                                                                                                                                                                                                                                                                                                                             \n",
       "2  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lapland —  /  \"                                                                                                                                                                                                                                                                                                                       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f'Train size: {len(train)} | Test size: {len(test)}')\n",
    "display(train.head(3))\n",
    "display(test.head(3))\n",
    "# display(sample_subm_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a look at the overall statistics of the data. Also included another label named 'none' to represent comments that have no negative sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>none</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>159571</td>\n",
       "      <td>159571</td>\n",
       "      <td>159571</td>\n",
       "      <td>159571</td>\n",
       "      <td>159571</td>\n",
       "      <td>159571</td>\n",
       "      <td>159571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0958445</td>\n",
       "      <td>0.00999555</td>\n",
       "      <td>0.0529482</td>\n",
       "      <td>0.00299553</td>\n",
       "      <td>0.0493636</td>\n",
       "      <td>0.00880486</td>\n",
       "      <td>0.898321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.294379</td>\n",
       "      <td>0.0994771</td>\n",
       "      <td>0.223931</td>\n",
       "      <td>0.0546496</td>\n",
       "      <td>0.216627</td>\n",
       "      <td>0.0934205</td>\n",
       "      <td>0.302226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>counts</th>\n",
       "      <td>159571</td>\n",
       "      <td>159571</td>\n",
       "      <td>159571</td>\n",
       "      <td>159571</td>\n",
       "      <td>159571</td>\n",
       "      <td>159571</td>\n",
       "      <td>159571</td>\n",
       "      <td>159571</td>\n",
       "      <td>159571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uniques</th>\n",
       "      <td>159571</td>\n",
       "      <td>159571</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missing</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missing_perc</th>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "      <td>0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>types</th>\n",
       "      <td>unique</td>\n",
       "      <td>unique</td>\n",
       "      <td>bool</td>\n",
       "      <td>bool</td>\n",
       "      <td>bool</td>\n",
       "      <td>bool</td>\n",
       "      <td>bool</td>\n",
       "      <td>bool</td>\n",
       "      <td>bool</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id comment_text      toxic severe_toxic    obscene  \\\n",
       "count         NaN     NaN          159571     159571       159571      \n",
       "mean          NaN     NaN          0.0958445  0.00999555   0.0529482   \n",
       "std           NaN     NaN          0.294379   0.0994771    0.223931    \n",
       "min           NaN     NaN          0          0            0           \n",
       "25%           NaN     NaN          0          0            0           \n",
       "50%           NaN     NaN          0          0            0           \n",
       "75%           NaN     NaN          0          0            0           \n",
       "max           NaN     NaN          1          1            1           \n",
       "counts        159571  159571       159571     159571       159571      \n",
       "uniques       159571  159571       2          2            2           \n",
       "missing       0       0            0          0            0           \n",
       "missing_perc  0%      0%           0%         0%           0%          \n",
       "types         unique  unique       bool       bool         bool        \n",
       "\n",
       "                  threat     insult identity_hate      none  \n",
       "count         159571      159571     159571        159571    \n",
       "mean          0.00299553  0.0493636  0.00880486    0.898321  \n",
       "std           0.0546496   0.216627   0.0934205     0.302226  \n",
       "min           0           0          0             0         \n",
       "25%           0           0          0             1         \n",
       "50%           0           0          0             1         \n",
       "75%           0           0          0             1         \n",
       "max           1           1          1             1         \n",
       "counts        159571      159571     159571        159571    \n",
       "uniques       2           2          2             2         \n",
       "missing       0           0          0             0         \n",
       "missing_perc  0%          0%         0%            0%        \n",
       "types         bool        bool       bool          bool      "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "train['none'] = 1 - train[label_cols].max(axis=1)\n",
    "\n",
    "DataFrameSummary(train).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://drive.google.com/file/d/0B1yuv8YaUVlZZ1RzMFJmc1ZsQmM/view\n",
    "# Aphost lookup dict\n",
    "APPO = {\n",
    "\"aren't\" : \"are not\",\n",
    "\"can't\" : \"cannot\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"I would\",\n",
    "\"i'd\" : \"I had\",\n",
    "\"i'll\" : \"I will\",\n",
    "\"i'm\" : \"I am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"I have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who're\" : \"who are\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\":\" will\",\n",
    "\"didn't\": \"did not\",\n",
    "\"tryin'\":\"trying\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.2.5.tar.gz (1.2MB)\n",
      "Requirement already satisfied: six in c:\\users\\hafid\\dropbox\\3.selfstudy\\ml_lab\\anaconda3\\envs\\fastai\\lib\\site-packages (from nltk)\n",
      "Building wheels for collected packages: nltk\n",
      "  Running setup.py bdist_wheel for nltk: started\n",
      "  Running setup.py bdist_wheel for nltk: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\hafid\\AppData\\Local\\pip\\Cache\\wheels\\18\\9c\\1f\\276bc3f421614062468cb1c9d695e6086d0c73d67ea363c501\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.2.5\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hafid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hafid\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "lem = WordNetLemmatizer()\n",
    "tweet_tokenizer=TweetTokenizer()\n",
    "\n",
    "def clean(comment):\n",
    "    \"\"\"\n",
    "    This function receives comments and returns clean word-list\n",
    "    \"\"\"\n",
    "    #Convert to lower case , so that Hi and hi are the same\n",
    "    comment=comment.lower()\n",
    "    #remove \\n\n",
    "    comment=re.sub(\"\\\\n\",\"\",comment)\n",
    "    # remove leaky elements like ip,user\n",
    "    comment=re.sub(\"\\d{1,3}.\\d{1,3}.\\d{1,3}.\\d{1,3}\",\"\",comment)\n",
    "    #removing usernames\n",
    "    comment=re.sub(\"\\[\\[.*\\]\",\"\",comment)\n",
    "    \n",
    "    #Split the sentences into words\n",
    "    words=tweet_tokenizer.tokenize(comment)\n",
    "    \n",
    "    # (')aphostophe  replacement (ie)   you're --> you are  \n",
    "    # ( basic dictionary lookup : master dictionary present in a hidden block of code)\n",
    "    words=[APPO[word] if word in APPO else word for word in words]\n",
    "    words=[lem.lemmatize(word, \"v\") for word in words]\n",
    "    words = [w for w in words if not w in eng_stopwords]\n",
    "    \n",
    "    clean_sent=\" \".join(words)\n",
    "    # remove any non alphanum,digit character\n",
    "    #clean_sent=re.sub(\"\\W+\",\" \",clean_sent)\n",
    "    #clean_sent=re.sub(\"  \",\" \",clean_sent)\n",
    "    return(clean_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['clean_comment_text'] = train.comment_text.apply(lambda x: clean(x))\n",
    "test['clean_comment_text'] = test.comment_text.apply(lambda x: clean(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Dataset and ModelData Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextMultiLabelDataset(torchtext.data.Dataset):\n",
    "    def __init__(self, df, tt_text_field, tt_label_field, txt_col, lbl_cols, **kwargs):\n",
    "        # torchtext Field objects\n",
    "        fields = [('text', tt_text_field)]\n",
    "        for l in lbl_cols: fields.append((l, tt_label_field))\n",
    "            \n",
    "        is_test = False if lbl_cols[0] in df.columns else True\n",
    "        n_labels = len(lbl_cols)\n",
    "        \n",
    "        examples = []\n",
    "        for idx, row in df.iterrows():\n",
    "            if not is_test:\n",
    "                lbls = [ row[l] for l in lbl_cols ]\n",
    "            else:\n",
    "                lbls = [0.0] * n_labels\n",
    "                \n",
    "            txt = str(row[txt_col])\n",
    "            examples.append(data.Example.fromlist([txt]+lbls, fields))\n",
    "                            \n",
    "        super().__init__(examples, fields, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_key(example): \n",
    "        return len(example.text)\n",
    "    \n",
    "    @classmethod\n",
    "    def splits(cls, text_field, label_field, train_df, txt_col, lbl_cols, val_df=None, test_df=None, **kwargs):\n",
    "        # build train, val, and test data\n",
    "        train_data, val_data, test_data = (None, None, None)\n",
    "        \n",
    "        if train_df is not None: \n",
    "            train_data = cls(train_df.copy(), text_field, label_field, txt_col, lbl_cols, **kwargs)\n",
    "        if val_df is not None: \n",
    "            val_data = cls(val_df.copy(), text_field, label_field, txt_col, lbl_cols, **kwargs)\n",
    "        if test_df is not None: \n",
    "            test_data = cls(test_df.copy(), text_field, label_field, txt_col, lbl_cols, **kwargs)\n",
    "\n",
    "        return tuple(d for d in (train_data, val_data, test_data) if d is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextMultiLabelDataLoader():\n",
    "    def __init__(self, src, x_fld, y_flds, y_dtype='torch.cuda.FloatTensor'):\n",
    "        self.src, self.x_fld, self.y_flds = src, x_fld, y_flds\n",
    "        self.y_dtype = y_dtype\n",
    "\n",
    "    def __len__(self): return len(self.src)#-1\n",
    "\n",
    "    def __iter__(self):\n",
    "        it = iter(self.src)\n",
    "        for i in range(len(self)):\n",
    "            b = next(it)\n",
    "            \n",
    "            if (len(self.y_flds) > 1):\n",
    "                targ = [ getattr(b, y) for y in self.y_flds ] \n",
    "                targ = torch.stack(targ, dim=1).type(self.y_dtype)\n",
    "            else: \n",
    "                targ = getattr(b, self.y_flds[0])\n",
    "                targ = targ.type(self.y_dtype)\n",
    "\n",
    "            yield getattr(b, self.x_fld), targ\n",
    "\n",
    "class TextMultiLabelData(ModelData):\n",
    "\n",
    "    @classmethod\n",
    "    def from_splits(cls, path, splits, bs, text_name='text', label_names=['label'], \n",
    "                    target_dtype='torch.cuda.FloatTensor'):\n",
    "        \n",
    "        text_fld = splits[0].fields[text_name]\n",
    "        \n",
    "        label_flds = []\n",
    "        if (len(label_names) == 1): \n",
    "            label_fld = splits[0].fields[label_names[0]]\n",
    "            label_flds.append(label_fld)\n",
    "            if (label_fld.use_vocab): \n",
    "                label_fld.build_vocab(splits[0])\n",
    "                target_dtype = 'torch.cuda.LongTensor'\n",
    "        else:\n",
    "            for n in label_names:\n",
    "                label_fld = splits[0].fields[n]\n",
    "                label_flds.append(label_fld)\n",
    "\n",
    "        iters = torchtext.data.BucketIterator.splits(splits, batch_size=bs)\n",
    "        trn_iter,val_iter,test_iter = iters[0],iters[1],None\n",
    "        test_dl = None\n",
    "        if len(iters) == 3:\n",
    "            test_iter = iters[2]\n",
    "            test_dl = TextMultiLabelDataLoader(test_iter, text_name, label_names, target_dtype)\n",
    "        trn_dl = TextMultiLabelDataLoader(trn_iter, text_name, label_names, target_dtype)\n",
    "        val_dl = TextMultiLabelDataLoader(val_iter, text_name, label_names, target_dtype)\n",
    "\n",
    "        obj = cls.from_dls(path, trn_dl, val_dl, test_dl)\n",
    "        obj.bs = bs\n",
    "        obj.pad_idx = text_fld.vocab.stoi[text_fld.pad_token]\n",
    "        obj.nt = len(text_fld.vocab)\n",
    "\n",
    "        # if multiple labels, assume the # of classes = the # of labels \n",
    "        if (len(label_names) > 1):\n",
    "            c = len(label_names)\n",
    "        # if label has a vocab, assume the vocab represents the # of classes\n",
    "        elif (hasattr(label_flds[0], 'vocab')): \n",
    "            c = len(label_flds[0].vocab)\n",
    "        else:\n",
    "            c = 1\n",
    "            \n",
    "        obj.c = c\n",
    "\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "def tokenizer(s): return re_tok.sub(r' \\1 ', s).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz = 8 #16 #32 #64\n",
    "\n",
    "max_tokens = 20000 # max number of words based on frequency\n",
    "max_len = None #100      # max length of each comment to look at\n",
    "\n",
    "n_hidden = 256\n",
    "n_fac = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac, bsz, \n",
    "                 fc_szs=[], fc_drops=[], n_lstm_hidden=256, n_lstm_layers=1, out_sz=1,\n",
    "                 lstm_drop=0.5, is_multi=False, y_range=None, use_bn=False):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size, self.out_sz = vocab_size, out_sz\n",
    "        self.n_lstm_layers, self.n_lstm_hidden = n_lstm_layers, n_lstm_hidden\n",
    "        \n",
    "        self.is_multi = is_multi\n",
    "        self.y_range = y_range\n",
    "        self.use_bn = use_bn\n",
    "        \n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.e.weight.data.uniform_(-0.1, 0.1)\n",
    "        \n",
    "        self.rnn = nn.LSTM(n_fac, n_lstm_hidden, n_lstm_layers, dropout=lstm_drop)\n",
    "        \n",
    "        fc_szs = [n_lstm_hidden] + fc_szs\n",
    "        \n",
    "        self.linears, self.linear_drops, self.linear_bns = [], [], []\n",
    "        if (len(fc_szs) > 1):\n",
    "            self.linears = nn.ModuleList(\n",
    "                [ nn.Linear(fc_szs[idx], sz) for idx, sz in enumerate(fc_szs[1:]) ]\n",
    "            )\n",
    "            \n",
    "            self.linear_bns = nn.ModuleList(\n",
    "                [ nn.BatchNorm1d(sz) for sz in fc_szs[1:] ]\n",
    "            )\n",
    "            \n",
    "            for l in self.linears: kaiming_normal(l.weight.data)\n",
    "                \n",
    "        if (len(fc_drops) > 0):\n",
    "            self.linear_drops = nn.ModuleList([ nn.Dropout(d) for d in fc_drops ])\n",
    "                \n",
    "        self.outp = nn.Linear(fc_szs[-1], out_sz)\n",
    "        kaiming_normal(self.outp.weight.data)\n",
    "            \n",
    "        self.h = self.init_hidden(bsz)\n",
    "        \n",
    "    def forward(self, words):\n",
    "        bsz = words[0].size(0)\n",
    "        if (self.h[0].size(1) != bsz): self.h = self.init_hidden(bsz)\n",
    "            \n",
    "        x, h = self.rnn(self.e(words), self.h)\n",
    "        self.h = repackage_var(h)\n",
    "        \n",
    "        x = x[-1]\n",
    "        \n",
    "        for l, d, b in zip(self.linears, self.linear_drops, self.linear_bns):\n",
    "            x = F.relu(l(x))\n",
    "            if (self.use_bn): x = b(x)\n",
    "            x = d(x)\n",
    "        \n",
    "        x = self.outp(x)\n",
    "\n",
    "        if (self.is_multi):\n",
    "            return F.sigmoid(x)\n",
    "        \n",
    "        if (not self.is_multi and self.out_sz > 1):\n",
    "            return F.log_softmax(x)\n",
    "        \n",
    "        if (self.y_range):\n",
    "            x = F.sigmoid(x)\n",
    "            x = x * (self.y_range[1] - self.y_range[0])\n",
    "            x = x + self.y_range[0]\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def init_hidden(self, bsz):\n",
    "        return(V(torch.zeros(self.n_lstm_layers, bsz, self.n_lstm_hidden)), \n",
    "               V(torch.zeros(self.n_lstm_layers, bsz, self.n_lstm_hidden)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi label classification\n",
    "Predict class probabilities where multiple classes can be true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159412, 159, 153164)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_col = 'comment_text'\n",
    "\n",
    "val_idxs = get_cv_idxs(len(train), val_pct=0.001)\n",
    "\n",
    "train_df =  train.drop(val_idxs)\n",
    "val_df = train.copy().iloc[val_idxs]\n",
    "\n",
    "len(train_df), len(val_df), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               object \n",
       "comment_text     object \n",
       "toxic            float32\n",
       "severe_toxic     float32\n",
       "obscene          float32\n",
       "threat           float32\n",
       "insult           float32\n",
       "identity_hate    float32\n",
       "none             float32\n",
       "dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ensure target fields are the correct datatype\n",
    "train_df[label_cols + ['none']] = train_df[label_cols + ['none']].astype(np.float32)\n",
    "val_df[label_cols + ['none']] = val_df[label_cols + ['none']].astype(np.float32)\n",
    "\n",
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since our label is essentially a OHE vector, set use_vocab=False for the label's torchtext field\n",
    "tt_TEXT = data.Field(sequential=True, tokenize=tokenizer, fix_length=max_len)\n",
    "tt_LABEL = data.Field(sequential=False, use_vocab=False,tensor_type=torch.cuda.FloatTensor) # use_vocab=False if labels are numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = TextMultiLabelDataset.splits(tt_TEXT, tt_LABEL, train_df, \n",
    "                                      txt_col, label_cols, val_df, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0,\n",
       " 0.0,\n",
       " \"Explanation Why the edits made under my username Hardcore Metallica Fan were reverted ? They weren ' t vandalisms , just closure on some GAs after I voted at New York Dolls FAC . And please don ' t remove the template from the talk page since I ' m retired now . 89 . 205 . 38 . 27\")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------ multi-label problem ------\n",
    "t = splits[0].examples[0]\n",
    "t.toxic, t.insult, ' '.join(t.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_TEXT.build_vocab(splits[0], max_size=max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19927, 20002, 6)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md = TextMultiLabelData.from_splits(PATH, splits, bsz, text_name='text', label_names=label_cols)\n",
    "\n",
    "len(md.trn_dl), md.nt, md.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(md.val_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 6]), Variable containing:\n",
       "  0  0  0  0  0  0\n",
       "  0  0  0  0  0  0\n",
       "  1  0  1  0  1  1\n",
       "  0  0  0  0  0  0\n",
       "  0  0  0  0  0  0\n",
       " [torch.cuda.FloatTensor of size 5x6 (GPU 0)])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.size(), y[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "md.val_dl.it.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "['<unk>', '<pad>', '.', ',', 'the', '\"', 'to', 'I', 'of', \"'\"]\n"
     ]
    }
   ],
   "source": [
    "print(tt_TEXT.vocab.stoi[tt_TEXT.pad_token])\n",
    "print(tt_TEXT.vocab.itos[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most common words: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('.', 679644),\n",
       " (',', 474397),\n",
       " ('the', 448846),\n",
       " ('\"', 392207),\n",
       " ('to', 291675),\n",
       " ('I', 224237),\n",
       " ('of', 221226),\n",
       " (\"'\", 219203),\n",
       " ('and', 212449),\n",
       " ('a', 203738)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('most common words: ')\n",
    "tt_TEXT.vocab.freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "least common words: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Gag01001', 1),\n",
       " ('Aberration', 1),\n",
       " ('1053', 1),\n",
       " ('Hanumakonda', 1),\n",
       " ('956CE', 1),\n",
       " ('automakers', 1),\n",
       " ('Boastful', 1),\n",
       " ('Superlatives', 1),\n",
       " ('Classifying', 1),\n",
       " ('CIU', 1)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('least common words: ')\n",
    "tt_TEXT.vocab.freqs.most_common()[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = LstmClassifier(md.nt, n_fac, bsz, [512, 256], [0.4, 0.2],\n",
    "                   n_hidden, n_lstm_layers=1, out_sz=md.c, is_multi=True, use_bn=True).cuda()\n",
    "\n",
    "lo = LayerOptimizer(optim.Adam, m, 1e-3, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e17edbd87ef48588b469c5ec4af2133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4%|██▎                                                             | 739/19927 [10:10<4:24:21,  1.21it/s, loss=0.131]"
     ]
    }
   ],
   "source": [
    "fit(m, md, 1, lo.opt, F.binary_cross_entropy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "123px",
    "width": "251px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
